{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-1. Import TensorFlow and numpy Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\peter\\anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-2.Activate a TensorFlow Interactive Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x19e580b1988>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.InteractiveSession()\n",
    "#tf.InteractiveSession.close()\n",
    "#tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-3. Defining Tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.zeros((2,2));\n",
    "b = tf.ones((2,2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-4. Sum the elements of the matrix (2D Tensor) across the horizontal axis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(b,reduction_indices = 1).eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-5. Check the shape of the Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_shape() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-6. Reshaping a Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(a,(1,4)).eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-7. Explicit evaluation in TensorFlow and difference with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_1:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ta = tf.zeros((2,2))\n",
    "print(ta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(ta.eval()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((2,2))\n",
    "print(a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-8.Defining TensorFlow Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(5)\n",
    "c= a*b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-9. TensorFlow Sesssion for executive of the Commands through Run and Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(c.eval())\n",
    "    print(sess.run(c)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-10a.Defining TensorFlow variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.ones((2,2),name='weights')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-10b.Initializing the variables after invoking the Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-11a.Defining the TensorFlow Variable with random initial values from Standard Normal Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = tf.Variable(tf.random.normal((2,2)),name='random_weights') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-11b.Invoking Session and displaying the Initial State of the variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02079094  1.0754267 ]\n",
      " [ 0.11389277 -0.7199395 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session()as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(rw)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-12.TensorFlow Variable State Update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "var_1 = tf.Variable(0,name='var_1')\n",
    "add_op = tf.add(var_1,tf.constant(1))\n",
    "upd_op = tf.assign(var_1,add_op)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5):\n",
    "        print(sess.run(upd_op)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-13. Displaying the TensorFlow Variable State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 12]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1)\n",
    "y = tf.constant(5)\n",
    "z = tf.constant(7)\n",
    "mul_x_y = x*y\n",
    "final_op = mul_x_y + z\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([mul_x_y,final_op])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-14.Converting a Numpy array to Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((3,3))\n",
    "b = tf.convert_to_tensor(a)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(b)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-15.PlaceHolders and Feed Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[10.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "inp1 = tf.placeholder(tf.float32,shape=(1,2))\n",
    "inp2 = tf.placeholder(tf.float32,shape=(2,1))\n",
    "output = tf.matmul(inp1,inp2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([output],feed_dict={inp1:[[1.,3.]],inp2:[[1],[3]]})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-16 . XOR implementation with Hidden layers having sigmoid activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Prediction: [[0.45279184]\n",
      " [0.4551609 ]\n",
      " [0.4154554 ]\n",
      " [0.41753754]]\n",
      "Weights from input to hidden layer: [[ 0.6894517  -0.5757006 ]\n",
      " [-0.07358292 -0.03825917]]\n",
      "Bias in the hidden layer: [-9.7440592e-05  4.5593402e-05]\n",
      "Weights from hidden layer to output layer: [[-0.6711084 ]\n",
      " [ 0.29097557]]\n",
      "Bias in the output layer: [0.00065013]\n",
      "Cost: 0.7022252\n",
      "Epoch  10000\n",
      "Prediction: [[0.51068115]\n",
      " [0.51634425]\n",
      " [0.4838612 ]\n",
      " [0.4890507 ]]\n",
      "Weights from input to hidden layer: [[ 0.6420843  -0.51364905]\n",
      " [-0.17471157  0.02230651]]\n",
      "Bias in the hidden layer: [0.04889339 0.0092715 ]\n",
      "Weights from hidden layer to output layer: [[-0.48663378]\n",
      " [ 0.25742042]]\n",
      "Bias in the output layer: [0.16268827]\n",
      "Cost: 0.6932912\n",
      "Epoch  20000\n",
      "Prediction: [[0.5077207 ]\n",
      " [0.5150706 ]\n",
      " [0.48499128]\n",
      " [0.49159035]]\n",
      "Weights from input to hidden layer: [[ 0.66161555 -0.48635176]\n",
      " [-0.24727294  0.04803667]]\n",
      "Bias in the hidden layer: [0.14073145 0.00203344]\n",
      "Weights from hidden layer to output layer: [[-0.4395397 ]\n",
      " [ 0.18993932]]\n",
      "Bias in the output layer: [0.17102763]\n",
      "Cost: 0.69306314\n",
      "Epoch  30000\n",
      "Prediction: [[0.50605816]\n",
      " [0.51565087]\n",
      " [0.48423588]\n",
      " [0.49246758]]\n",
      "Weights from input to hidden layer: [[ 0.7289742  -0.46988878]\n",
      " [-0.33659708  0.06222022]]\n",
      "Bias in the hidden layer: [ 0.25693077 -0.00450122]\n",
      "Weights from hidden layer to output layer: [[-0.43194172]\n",
      " [ 0.14138483]]\n",
      "Bias in the output layer: [0.19726443]\n",
      "Cost: 0.69276017\n",
      "Epoch  40000\n",
      "Prediction: [[0.50492316]\n",
      " [0.5197288 ]\n",
      " [0.4799533 ]\n",
      " [0.491505  ]]\n",
      "Weights from input to hidden layer: [[ 0.8710611  -0.45675346]\n",
      " [-0.47759023  0.06973414]]\n",
      "Bias in the hidden layer: [ 0.42449448 -0.00907577]\n",
      "Weights from hidden layer to output layer: [[-0.4871527 ]\n",
      " [ 0.10683238]]\n",
      "Bias in the output layer: [0.26103175]\n",
      "Cost: 0.69196427\n",
      "Epoch  50000\n",
      "Prediction: [[0.5025023 ]\n",
      " [0.53524387]\n",
      " [0.46550772]\n",
      " [0.4856054 ]]\n",
      "Weights from input to hidden layer: [[ 1.1980877  -0.44000712]\n",
      " [-0.77686924  0.07134271]]\n",
      "Bias in the hidden layer: [ 0.7215852  -0.01061354]\n",
      "Weights from hidden layer to output layer: [[-0.69355726]\n",
      " [ 0.09320895]]\n",
      "Bias in the output layer: [0.43038583]\n",
      "Cost: 0.68814707\n",
      "Epoch  60000\n",
      "Prediction: [[0.48287973]\n",
      " [0.60762817]\n",
      " [0.41888472]\n",
      " [0.45770037]]\n",
      "Weights from input to hidden layer: [[ 2.0795734  -0.39080903]\n",
      " [-1.6313633   0.05495445]]\n",
      "Bias in the hidden layer: [ 1.3216947  -0.00262657]\n",
      "Weights from hidden layer to output layer: [[-1.3758526 ]\n",
      " [ 0.14056228]]\n",
      "Bias in the output layer: [0.9474886]\n",
      "Cost: 0.65994203\n",
      "Epoch  70000\n",
      "Prediction: [[0.42111993]\n",
      " [0.7873715 ]\n",
      " [0.3595312 ]\n",
      " [0.3949815 ]]\n",
      "Weights from input to hidden layer: [[ 3.596993   -0.13853922]\n",
      " [-3.1337197  -0.10490035]]\n",
      "Bias in the hidden layer: [2.2431948  0.04465638]\n",
      "Weights from hidden layer to output layer: [[-2.6680212]\n",
      " [ 0.3175997]]\n",
      "Bias in the output layer: [1.9315339]\n",
      "Cost: 0.5777914\n",
      "Epoch  80000\n",
      "Prediction: [[0.36893466]\n",
      " [0.8742944 ]\n",
      " [0.42047656]\n",
      " [0.32765996]]\n",
      "Weights from input to hidden layer: [[ 4.7514415  0.9714226]\n",
      " [-4.298947  -1.244579 ]]\n",
      "Bias in the hidden layer: [ 3.0077143  -0.14430812]\n",
      "Weights from hidden layer to output layer: [[-3.9801702]\n",
      " [ 1.7330725]]\n",
      "Bias in the output layer: [2.451879]\n",
      "Cost: 0.46451038\n",
      "Epoch  90000\n",
      "Prediction: [[0.13807933]\n",
      " [0.90293854]\n",
      " [0.8329984 ]\n",
      " [0.10913751]]\n",
      "Weights from input to hidden layer: [[ 5.486687   3.297432 ]\n",
      " [-5.355719  -3.6721547]]\n",
      "Bias in the hidden layer: [ 3.1695647 -1.6492008]\n",
      "Weights from hidden layer to output layer: [[-5.7160425]\n",
      " [ 5.4143667]]\n",
      "Bias in the output layer: [2.7813075]\n",
      "Cost: 0.13724542\n",
      "Final Prediction [[0.06089826]\n",
      " [0.93945396]\n",
      " [0.9341196 ]\n",
      " [0.05041172]]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "#XOR  implementation in Tensorflow with hidden layers being sigmoid to introduce Non-Linearity\n",
    "#----------------------------------------------------------------------------------------------\n",
    "import tensorflow.compat.v1 as tf\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Create placeholders for training input and output labels\n",
    "#----------------------------------------------------------------------------------------------\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\")\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#Define the weights to the hidden and output layerrespectively. \n",
    "#----------------------------------------------------------------------------------------------\n",
    "w1 = tf.Variable(tf.random.uniform([2,2], -1, 1), name=\"Weights1\")\n",
    "w2 = tf.Variable(tf.random.uniform([2,1], -1, 1), name=\"Weights2\")\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Define the bias to the hidden and output layers respectively\n",
    "#----------------------------------------------------------------------------------------------\n",
    "b1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Define the final output through forward pass\n",
    "#----------------------------------------------------------------------------------------------\n",
    "z2 = tf.sigmoid(tf.matmul(x_, w1) + b1)\n",
    "pred = tf.sigmoid(tf.matmul(z2,w2) + b2)\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#Define the Cross-entropy/Log-loss Cost function based on the output label y and the predicted \n",
    "#probability by the forward pass\n",
    "#----------------------------------------------------------------------------------------------\n",
    "cost = tf.reduce_mean(( (y_ * tf.math.log(pred)) + \n",
    "        ((1 - y_) * tf.math.log(1.0 - pred)) ) * -1)\n",
    "learning_rate = 0.01\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#Now that we have all that we need set up we will start the training\n",
    "#----------------------------------------------------------------------------------------------\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "XOR_Y = [[0],[1],[1],[0]]\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./Downloads/XOR1_logs\", sess.graph)\n",
    "\n",
    "sess.run(init)\n",
    "for i in range(100000):\n",
    "        sess.run(train_step, feed_dict={x_: XOR_X, y_: XOR_Y})\n",
    "        if i % 10000 == 0:\n",
    "            print('Epoch ', i)\n",
    "            print('Prediction:', sess.run(pred,feed_dict={x_: XOR_X, y_: XOR_Y}))\n",
    "            print('Weights from input to hidden layer:', sess.run(w1))\n",
    "            print('Bias in the hidden layer:', sess.run(b1))\n",
    "            print('Weights from hidden layer to output layer:', sess.run(w2))\n",
    "            print('Bias in the output layer:', sess.run(b2))\n",
    "            print('Cost:', sess.run(cost, feed_dict={x_: XOR_X, y_: XOR_Y}))\n",
    "#----------------------------------------------------------------------------------------------        \n",
    "print('Final Prediction', sess.run(pred, feed_dict={x_: XOR_X, y_: XOR_Y}))\n",
    "#----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-17 . XOR implementation with linear activation functionsin hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Prediction: [[0.49948043]\n",
      " [0.37923428]\n",
      " [0.78579575]\n",
      " [0.69190615]]\n",
      "Weights from input to hidden layer: [[-0.9714017  -0.7398904 ]\n",
      " [ 0.6609279   0.10022276]]\n",
      "Bias in the hidden layer: [0.00053427 0.00088065]\n",
      "Weights from hidden layer to output layer: [[-0.5938999 ]\n",
      " [-0.97978026]]\n",
      "Bias in the output layer: [-0.00089802]\n",
      "Cost: 0.7700297\n",
      "Epoch  10000\n",
      "Prediction: [[0.4968651]\n",
      " [0.5001328]\n",
      " [0.4994594]\n",
      " [0.5027271]]\n",
      "Weights from input to hidden layer: [[-0.805877   -0.12737314]\n",
      " [ 0.61925775  0.06553549]]\n",
      "Bias in the hidden layer: [0.05185636 0.02466165]\n",
      "Weights from hidden layer to output layer: [[ 0.08997223]\n",
      " [-0.65071726]]\n",
      "Bias in the output layer: [-0.00115765]\n",
      "Cost: 0.69315594\n",
      "Epoch  20000\n",
      "Prediction: [[0.49969393]\n",
      " [0.50001395]\n",
      " [0.49994636]\n",
      " [0.5002664 ]]\n",
      "Weights from input to hidden layer: [[-0.8077733  -0.11364941]\n",
      " [ 0.61676204  0.08362014]]\n",
      "Bias in the hidden layer: [0.05256703 0.01953154]\n",
      "Weights from hidden layer to output layer: [[ 0.09021827]\n",
      " [-0.65011895]]\n",
      "Bias in the output layer: [0.0067309]\n",
      "Cost: 0.69314724\n",
      "Epoch  30000\n",
      "Prediction: [[0.49996924]\n",
      " [0.5000009 ]\n",
      " [0.49999523]\n",
      " [0.5000269 ]]\n",
      "Weights from input to hidden layer: [[-0.8078935  -0.11230087]\n",
      " [ 0.6165699   0.08538945]]\n",
      "Bias in the hidden layer: [0.05263834 0.0190319 ]\n",
      "Weights from hidden layer to output layer: [[ 0.0902408 ]\n",
      " [-0.65011895]]\n",
      "Bias in the output layer: [0.00749987]\n",
      "Cost: 0.6931472\n",
      "Epoch  40000\n",
      "Prediction: [[0.4999969 ]\n",
      " [0.49999988]\n",
      " [0.49999964]\n",
      " [0.5000026 ]]\n",
      "Weights from input to hidden layer: [[-0.8078935  -0.11215788]\n",
      " [ 0.6165699   0.08556576]]\n",
      "Bias in the hidden layer: [0.05263834 0.01898311]\n",
      "Weights from hidden layer to output layer: [[ 0.0902408 ]\n",
      " [-0.65011895]]\n",
      "Bias in the output layer: [0.00757879]\n",
      "Cost: 0.6931472\n",
      "Epoch  50000\n",
      "Prediction: [[0.49999768]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000226]]\n",
      "Weights from input to hidden layer: [[-0.8078935  -0.11215518]\n",
      " [ 0.6165699   0.08556973]]\n",
      "Bias in the hidden layer: [0.05263834 0.01898217]\n",
      "Weights from hidden layer to output layer: [[ 0.0902408 ]\n",
      " [-0.65011895]]\n",
      "Bias in the output layer: [0.00758119]\n",
      "Cost: 0.6931471\n",
      "Epoch  60000\n",
      "Prediction: [[0.49999768]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000226]]\n",
      "Weights from input to hidden layer: [[-0.8078935  -0.11215518]\n",
      " [ 0.6165699   0.08556973]]\n",
      "Bias in the hidden layer: [0.05263834 0.01898217]\n",
      "Weights from hidden layer to output layer: [[ 0.0902408 ]\n",
      " [-0.65011895]]\n",
      "Bias in the output layer: [0.00758119]\n",
      "Cost: 0.6931471\n",
      "Epoch  70000\n",
      "Prediction: [[0.49999768]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000226]]\n",
      "Weights from input to hidden layer: [[-0.8078935  -0.11215518]\n",
      " [ 0.6165699   0.08556973]]\n",
      "Bias in the hidden layer: [0.05263834 0.01898217]\n",
      "Weights from hidden layer to output layer: [[ 0.0902408 ]\n",
      " [-0.65011895]]\n",
      "Bias in the output layer: [0.00758119]\n",
      "Cost: 0.6931471\n",
      "Epoch  80000\n",
      "Prediction: [[0.49999768]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000226]]\n",
      "Weights from input to hidden layer: [[-0.8078935  -0.11215518]\n",
      " [ 0.6165699   0.08556973]]\n",
      "Bias in the hidden layer: [0.05263834 0.01898217]\n",
      "Weights from hidden layer to output layer: [[ 0.0902408 ]\n",
      " [-0.65011895]]\n",
      "Bias in the output layer: [0.00758119]\n",
      "Cost: 0.6931471\n",
      "Epoch  90000\n",
      "Prediction: [[0.49999768]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000226]]\n",
      "Weights from input to hidden layer: [[-0.8078935  -0.11215518]\n",
      " [ 0.6165699   0.08556973]]\n",
      "Bias in the hidden layer: [0.05263834 0.01898217]\n",
      "Weights from hidden layer to output layer: [[ 0.0902408 ]\n",
      " [-0.65011895]]\n",
      "Bias in the output layer: [0.00758119]\n",
      "Cost: 0.6931471\n",
      "Final Prediction [[0.49999768]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000226]]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "#XOR  implementation in Tensorflow with linear activation for hidden layers \n",
    "#----------------------------------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Create placeholders for training input and output labels\n",
    "#----------------------------------------------------------------------------------------------\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\")\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#Define the weights to the hidden and output layer respectively. \n",
    "#----------------------------------------------------------------------------------------------\n",
    "w1 = tf.Variable(tf.random_uniform([2,2], -1, 1), name=\"Weights1\")\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -1, 1), name=\"Weights2\")\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Define the bias to the hideen and output layers respectively\n",
    "#----------------------------------------------------------------------------------------------\n",
    "b1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Define the final output through forward pass\n",
    "#----------------------------------------------------------------------------------------------\n",
    "z2 = tf.matmul(x_, w1) + b1\n",
    "pred = tf.sigmoid(tf.matmul(z2,w2) + b2)\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#Define the Cross-entropy/Log-loss Cost function based on the output label y and the predicted \n",
    "#probability by the forward pass\n",
    "#----------------------------------------------------------------------------------------------\n",
    "cost = tf.reduce_mean(( (y_ * tf.log(pred)) + \n",
    "        ((1 - y_) * tf.log(1.0 - pred)) ) * -1)\n",
    "learning_rate = 0.01\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#Now that we have all that we need set up we will start the training\n",
    "#----------------------------------------------------------------------------------------------\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "XOR_Y = [[0],[1],[1],[0]]\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(100000):\n",
    "        sess.run(train_step, feed_dict={x_: XOR_X, y_: XOR_Y})\n",
    "        if i % 10000 == 0:\n",
    "            print('Epoch ', i)\n",
    "            print('Prediction:', sess.run(pred,feed_dict={x_: XOR_X, y_: XOR_Y}))\n",
    "            print('Weights from input to hidden layer:', sess.run(w1))\n",
    "            print('Bias in the hidden layer:', sess.run(b1))\n",
    "            print('Weights from hidden layer to output layer:', sess.run(w2))\n",
    "            print('Bias in the output layer:', sess.run(b2))\n",
    "            print('Cost:', sess.run(cost, feed_dict={x_: XOR_X, y_: XOR_Y}))\n",
    "#----------------------------------------------------------------------------------------------        \n",
    "print('Final Prediction', sess.run(pred, feed_dict={x_: XOR_X, y_: XOR_Y}))\n",
    "#----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Listing 2-18. Linear Regression implementation in TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('MSE in training:',cost_trace[-1])? (Temp/ipykernel_22520/644485956.py, line 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\peter\\AppData\\Local\\Temp/ipykernel_22520/644485956.py\"\u001b[1;36m, line \u001b[1;32m100\u001b[0m\n\u001b[1;33m    print 'MSE in training:',cost_trace[-1]\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('MSE in training:',cost_trace[-1])?\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Importing TensorFlow, Numpy and the Boston Housing price dataset\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to load the Boston data set\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "def read_infile():\n",
    "    data = load_boston()\n",
    "    features = np.array(data.data)\n",
    "    target = np.array(data.target)\n",
    "    return features,target\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Normalize the features by Z scaling i.e. subract form each feature value its mean and then divide by its \n",
    "# standard deviation. Accelerates Gradient Descent.\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "def feature_normalize(data):\n",
    "    mu = np.mean(data,axis=0)\n",
    "    std = np.std(data,axis=0)\n",
    "    return (data - mu)/std\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Append the feature for the bias term.\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "def append_bias(features,target):\n",
    "    n_samples = features.shape[0]\n",
    "    n_features = features.shape[1]\n",
    "    intercept_feature  = np.ones((n_samples,1))\n",
    "    X = np.concatenate((features,intercept_feature),axis=1)\n",
    "    X = np.reshape(X,[n_samples,n_features +1])\n",
    "    Y = np.reshape(target,[n_samples,1])\n",
    "    return X,Y\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Execute the functions to read, normalize and add append bias term to the data\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "features,target = read_infile()\n",
    "z_features = feature_normalize(features)\n",
    "X_input,Y_input = append_bias(z_features,target)\n",
    "num_features = X_input.shape[1]\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create TensorFlow ops for placeholders, weights and weight initialization\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,num_features])\n",
    "Y = tf.placeholder(tf.float32,[None,1])\n",
    "w = tf.Variable(tf.random_normal((num_features,1)),name='weights')\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define the different TensforFlow ops and input parameters for Cost and Optimization.\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "cost_trace = []\n",
    "pred = tf.matmul(X,w)\n",
    "error = pred - Y\n",
    "cost = tf.reduce_mean(tf.square(error))\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Execute the gradient descent learning\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in xrange(num_epochs):\n",
    "        sess.run(train_op,feed_dict={X:X_input,Y:Y_input})\n",
    "        cost_trace.append(sess.run(cost,feed_dict={X:X_input,Y:Y_input}))\n",
    "    error_ = sess.run(error,{X:X_input,Y:Y_input}) \n",
    "    pred_ = sess.run(pred,{X:X_input}) \n",
    "\n",
    "print 'MSE in training:',cost_trace[-1] \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-18a. Linear Regression Cost plot over Epochs or iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(cost_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-18b. Linear Regression Actual House Price vs Predicted House Price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Plot the Predicted house Prices vs the Actual House Prices\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(Y_input,pred_)\n",
    "ax.set_xlabel('Actual House price')\n",
    "ax.set_ylabel('Predicted House price') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-19. Multiclass Classification with Softmax function using Full Batch Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Import the required libraries\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to Read the MNIST dataset along with the labels\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "def read_infile():\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    train_X, train_Y,test_X, test_Y = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "    return train_X, train_Y,test_X, test_Y\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define the weights and biases for the neural network\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "def weights_biases_placeholder(n_dim,n_classes):\n",
    "    X = tf.placeholder(tf.float32,[None,n_dim])\n",
    "    Y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "    w = tf.Variable(tf.random_normal([n_dim,n_classes],stddev=0.01),name='weights')\n",
    "    b = tf.Variable(tf.random_normal([n_classes]),name='weights')\n",
    "    return X,Y,w,b\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define the forward pass\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "def forward_pass(w,b,X):\n",
    "    out = tf.matmul(X,w) + b\n",
    "    return out \n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define the cost function for the softmax unit\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "def multiclass_cost(out,Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out,labels=Y))\n",
    "    return cost\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define the initialiazation op\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "def init():\n",
    "    return tf.global_variables_initializer()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define the training op\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "    \n",
    "def train_op(learning_rate,cost):\n",
    "    op_train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    return op_train\n",
    "    \n",
    "\n",
    "\n",
    "train_X, train_Y,test_X, test_Y = read_infile()\n",
    "X,Y,w,b = weights_biases_placeholder(train_X.shape[1],train_Y.shape[1])\n",
    "out = forward_pass(w,b,X)\n",
    "cost = multiclass_cost(out,Y)\n",
    "learning_rate,epochs = 0.01,1000\n",
    "op_train = train_op(learning_rate,cost)\n",
    "init = init()\n",
    "loss_trace = []\n",
    "accuracy_trace = []\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Activate the Tensorflow session and execute the Stochastic gradient descent\n",
    "\n",
    "#---------------------------------------------------------------------------------------------- \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in xrange(epochs):\n",
    "        sess.run(op_train,feed_dict={X:train_X,Y:train_Y})\n",
    "        loss_ = sess.run(cost,feed_dict={X:train_X,Y:train_Y})\n",
    "        accuracy_ = np.mean(np.argmax(sess.run(out,feed_dict={X:train_X,Y:train_Y}),axis=1) == np.argmax(train_Y,axis=1))\n",
    "        loss_trace.append(loss_)\n",
    "        accuracy_trace.append(accuracy_)\n",
    "        if (((i+1) >= 100) and ((i+1) % 100 == 0 )) :\n",
    "            print 'Epoch:',(i+1),'loss:',loss_,'accuracy:',accuracy_\n",
    "                            \n",
    "    print 'Final training result:','loss:',loss_,'accuracy:',accuracy_    \n",
    "    loss_test = sess.run(cost,feed_dict={X:test_X,Y:test_Y})\n",
    "    test_pred = np.argmax(sess.run(out,feed_dict={X:test_X,Y:test_Y}),axis=1)\n",
    "    accuracy_test = np.mean(test_pred == np.argmax(test_Y,axis=1))\n",
    "    print 'Results on test dataset:','loss:',loss_test,'accuracy:',accuracy_test    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-19a. Display the Actual digits vs the Predicted digits along with the images of the actual digits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "f, a = plt.subplots(1, 10, figsize=(10, 2))\n",
    "print 'Actual digits:   ', np.argmax(test_Y[0:10],axis=1)\n",
    "print 'Predicted digits:',test_pred[0:10]\n",
    "print 'Actual images of the digits follow:'\n",
    "for i in range(10):\n",
    "        a[i].imshow(np.reshape(test_X[i],(28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-20. Multiclass Classification with Softmax function using Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_infile():\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    train_X, train_Y,test_X, test_Y = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "    return train_X, train_Y,test_X, test_Y\n",
    "\n",
    "\n",
    "def weights_biases_placeholder(n_dim,n_classes):\n",
    "    X = tf.placeholder(tf.float32,[None,n_dim])\n",
    "    Y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "    w = tf.Variable(tf.random_normal([n_dim,n_classes],stddev=0.01),name='weights')\n",
    "    b = tf.Variable(tf.random_normal([n_classes]),name='weights')\n",
    "    return X,Y,w,b\n",
    "\n",
    "def forward_pass(w,b,X):\n",
    "    out = tf.matmul(X,w) + b\n",
    "    return out \n",
    "\n",
    "def multiclass_cost(out,Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out,labels=Y))\n",
    "    return cost\n",
    "\n",
    "def init():\n",
    "    return tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "def train_op(learning_rate,cost):\n",
    "    op_train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    return op_train\n",
    "    \n",
    "\n",
    "\n",
    "train_X, train_Y,test_X, test_Y = read_infile()\n",
    "X,Y,w,b = weights_biases_placeholder(train_X.shape[1],train_Y.shape[1])\n",
    "out = forward_pass(w,b,X)\n",
    "cost = multiclass_cost(out,Y)\n",
    "learning_rate,epochs,batch_size = 0.01,1000,1000\n",
    "num_batches = train_X.shape[0]/batch_size\n",
    "op_train = train_op(learning_rate,cost)\n",
    "init = init()\n",
    "epoch_cost_trace = []\n",
    "epoch_accuracy_trace = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in xrange(epochs):\n",
    "        epoch_cost,epoch_accuracy = 0,0\n",
    "   \n",
    "        for j in xrange(num_batches):\n",
    "            sess.run(op_train,feed_dict={X:train_X[j*batch_size:(j+1)*batch_size],Y:train_Y[j*batch_size:(j+1)*batch_size]})\n",
    "            actual_batch_size = train_X[j*batch_size:(j+1)*batch_size].shape[0]\n",
    "            epoch_cost += actual_batch_size*sess.run(cost,feed_dict={X:train_X[j*batch_size:(j+1)*batch_size],Y:train_Y[j*batch_size:(j+1)*batch_size]})\n",
    "            \n",
    "            \n",
    "        epoch_cost = epoch_cost/float(train_X.shape[0])\n",
    "        epoch_accuracy = np.mean(np.argmax(sess.run(out,feed_dict={X:train_X,Y:train_Y}),axis=1) == np.argmax(train_Y,axis=1))\n",
    "        epoch_cost_trace.append(epoch_cost)\n",
    "        epoch_accuracy_trace.append(epoch_accuracy)\n",
    "        \n",
    "        if (((i +1) >= 100) and ((i+1) % 100 == 0 )) :\n",
    "            print 'Epoch:',(i+1),'Average loss:',epoch_cost,'accuracy:',epoch_accuracy\n",
    "                            \n",
    "    print 'Final epoch training results:','Average loss:',epoch_cost,'accuracy:',epoch_accuracy\n",
    "    loss_test = sess.run(cost,feed_dict={X:test_X,Y:test_Y})\n",
    "    test_pred = np.argmax(sess.run(out,feed_dict={X:test_X,Y:test_Y}),axis=1)\n",
    "    accuracy_test = np.mean(test_pred == np.argmax(test_Y,axis=1))\n",
    "    print 'Results on test dataset:','Average loss:',loss_test,'accuracy:',accuracy_test    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2-20a. Actual Digit vs Predicted digits for Softmax classification through Stochastic Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "f, a = plt.subplots(1, 10, figsize=(10, 2))\n",
    "print 'Actual digits:   ', np.argmax(test_Y[0:10],axis=1)\n",
    "print 'Predicted digits:',test_pred[0:10]\n",
    "print 'Actual images of the digits follow:'\n",
    "for i in range(10):\n",
    "        a[i].imshow(np.reshape(test_X[i],(28, 28)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
